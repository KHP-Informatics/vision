{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "REumE0BDQDdn"
   },
   "outputs": [],
   "source": [
    "# # preliminary installs and mounting of google drive\n",
    "#######################################################\n",
    "#!pip install transformers\n",
    "#!pip install ipynb\n",
    "#!pip install google.colab\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive', force_remount=True)\n",
    "#!pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install google.colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers\n",
    "# !pip install ipynb\n",
    "# !pip install nltk\n",
    "# !pip install gensim\n",
    "# !pip install scispacy\n",
    "# !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_md-0.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_RWQGjQFidZ"
   },
   "outputs": [],
   "source": [
    "## initializing path (are we on colab or azure machine?)\n",
    "import os \n",
    "ipynb_path = '/Users/lilifang/KCL/KCL_Angus/CRIS_COVID_Aurelie'\n",
    "#T:\\aurelie_mascio\\BERT_models\\violence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch\n",
    "# !pip install transformers -U\n",
    "# !pip install -U pip\n",
    "# !pip install packaging==20.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-transformers\n",
    "# !pip install --upgrade torch\n",
    "# !pip install --upgrade torchvision\n",
    "# !pip install --upgrade torchtext\n",
    "# !pip install --upgrade torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch==1.4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.24.3) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n",
      "2022-11-24 14:01:15.154286: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, trange\n",
    "from collections import Counter\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from transformers import BertTokenizer, BertConfig, BertForSequenceClassification, AdamW\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.0\n",
      "1.23.5\n",
      "1.4.4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "type object 'tqdm' has no attribute '__version__'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/tp/sl0jc6hj01lg3jcxlkglg7jw0000gn/T/ipykernel_24447/3482623394.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'tqdm' has no attribute '__version__'"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "print(np.__version__)\n",
    "print(pd.__version__)\n",
    "print(tqdm.__version__)\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Js31W5z5LInq",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# sys.path.insert(0, ipynb_path)\n",
    "# from ipynb.fs.full.NLP_utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%run NLP_utils.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_cat(labels, binary=False):\n",
    "    if not isinstance(labels, pd.Series):\n",
    "        labels = pd.Series(labels)\n",
    "    if binary:\n",
    "        main_val = labels.mode()[0]\n",
    "        other_val = [x for x in labels.unique() if x!= main_val]\n",
    "        labels = np.where(labels!=main_val,1,0)\n",
    "        corresp = pd.DataFrame([main_val, 'other'], columns=['old_label'])\n",
    "    elif np.issubdtype(labels.dtype, np.number) and (labels.min() >= 0):\n",
    "        print('labels already in a good format')\n",
    "        return {'labels':labels, 'categories':pd.DataFrame(labels.unique())}\n",
    "    else:\n",
    "        cats = labels.astype('category').cat\n",
    "        labels = cats.codes.astype('long') # convert annotations to integers\n",
    "        corresp = pd.DataFrame(cats.categories, columns=['old_label'])\n",
    "    corresp.index.rename('new_label', inplace=True)\n",
    "    print('labels have been transformed for the model:\\n\\n', corresp)\n",
    "    return {'labels':labels, 'categories':corresp}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO RUN K-FOLD VALIDATION\n",
    "def run_KFOLD(dataset, base_model, model_trainer, base_model_loader=None,\n",
    "             n_splits=10, random_state=42, n_epochs=5, **kwargs):\n",
    "    from torch.utils.data import DataLoader \n",
    "    kfold = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "    labels = pd.Series([1] * len(dataset))\n",
    "    # tracking variables\n",
    "    best_f1, fold_nb = 0, 0\n",
    "    stats, stats_classes = pd.DataFrame(), pd.DataFrame()\n",
    "    # run k fold\n",
    "    for train_ix, test_ix in kfold.split(labels, labels):\n",
    "        fold_nb +=1\n",
    "        # need to load each time otherwise remembers training from previous fold\n",
    "        if base_model_loader is not None:\n",
    "            base_model_tmp = base_model_loader(base_model, **kwargs)\n",
    "        else:\n",
    "            base_model_tmp = base_model\n",
    "        print('####################### RUNNING FOLD:', fold_nb)\n",
    "        train_dataset = torch.utils.data.Subset(dataset, train_ix)\n",
    "        val_dataset = torch.utils.data.Subset(dataset, test_ix)\n",
    "        print(type(train_dataset), ' train set:', len(train_dataset), ' test set:',len(val_dataset))\n",
    "        train_dataloader, validation_dataloader = create_dataloader(train_dataset, val_dataset)\n",
    "        del train_dataset, val_dataset\n",
    "        print('training and evaluating model')\n",
    "        res = model_trainer(base_model_tmp, train_dataloader, validation_dataloader, n_epochs=n_epochs, output_dir=None)\n",
    "        del base_model_tmp, train_dataloader, validation_dataloader\n",
    "        # store perf metrics and model\n",
    "        stats_tmp = pd.DataFrame.from_dict(res['stats'],orient='index', columns=['value'])\n",
    "        stats_tmp['fold'] = fold_nb\n",
    "        stats = pd.concat([stats, stats_tmp])\n",
    "        res['stats_classes']['fold'] = fold_nb\n",
    "        stats_classes = pd.concat([stats_classes, res['stats_classes']])\n",
    "        if res['stats']['f1'] >= best_f1:\n",
    "            best_f1 = res['stats']['f1']\n",
    "            res_to_save = res\n",
    "        del res\n",
    "            \n",
    "        \n",
    "    print('best F1 score obtained across splits: {:.3f}'.format(best_f1))\n",
    "    return {'stats':stats, 'stats_classes':stats_classes, 'model':res_to_save['model']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the DataLoaders for our training and validation sets.\n",
    "# For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32.\n",
    "def create_dataloader(train_dataset, val_dataset, batch_size = 32):\n",
    "    # We'll take training samples in random order. \n",
    "    train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler = RandomSampler(train_dataset), # Select batches randomly\n",
    "                batch_size = batch_size # Trains with this batch size.\n",
    "            )\n",
    "    # For validation the order doesn't matter, so we'll just read them sequentially.\n",
    "    validation_dataloader = DataLoader(\n",
    "                val_dataset, # The validation samples.\n",
    "                sampler = SequentialSampler(val_dataset), # Pull out batches sequentially.\n",
    "                batch_size = batch_size # Evaluate with this batch size.\n",
    "            )\n",
    "    \n",
    "    return [train_dataloader, validation_dataloader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jr2vZkQiLInx"
   },
   "source": [
    "# FUNCTIONS TO PREP DATASET / TRAIN BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JfRaDV1JLIny"
   },
   "outputs": [],
   "source": [
    "def prep_BERT_dataset(sentences, labels=None, BERT_tokenizer='bert-base-uncased', MAX_TKN_LEN=511, debug=False):\n",
    "    sentences = pd.Series(sentences)\n",
    "    # load relevant data and add special tokens for BERT to work properly\n",
    "    sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
    "    if labels is not None:\n",
    "        labels = convert_to_cat(labels, binary=False)['labels']\n",
    "    else:\n",
    "        labels = pd.Series([1] * len(sentences))\n",
    "    if debug: print(sentences[0])\n",
    "    \n",
    "    # Tokenize with BERT tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_tokenizer, do_lower_case=True)\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    # seems there are issues for sequence lengths > 512. TO CHECK\n",
    "    # https://github.com/huggingface/transformers/issues/2446\n",
    "    # !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "    if MAX_TKN_LEN is not None:\n",
    "        print('cutting the length of tokens to', MAX_TKN_LEN)\n",
    "        tokenized_texts = [tokenizer.tokenize(sent)[0:511] for sent in sentences]\n",
    "    else:\n",
    "        tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
    "    if debug:\n",
    "        print (\"Tokenize the first sentence:\")\n",
    "        print (len(tokenized_texts), len(input_ids), len(input_ids[0]))\n",
    "        print (tokenized_texts[0], input_ids[0])\n",
    "    \n",
    "    # add paddding to input_ids\n",
    "    input_ids_padded = pad_sequence([torch.tensor(i) for i in input_ids]).transpose(0,1)\n",
    "    if debug: print(input_ids_padded.size(), len(input_ids_padded))\n",
    "    \n",
    "    # Create attention masks\n",
    "    attention_masks = []\n",
    "    # Create a mask of 1s for each token followed by 0s for padding\n",
    "    for seq in input_ids_padded:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "    \n",
    "    # create dataset\n",
    "    dataset = TensorDataset(input_ids_padded, torch.tensor(attention_masks), torch.tensor(labels))\n",
    "    num_labels = labels.nunique()\n",
    "    return {'dataset': dataset, 'num_labels': num_labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOEjW5jULIn4",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_BERT(model, train_dataloader, validation_dataloader, n_epochs=5, output_dir=None):\n",
    "    ###################################################################################\n",
    "    # BERT fine-tuning parameters\n",
    "    device = get_device()\n",
    "    try:\n",
    "        model.cuda()\n",
    "    except:\n",
    "        device = None\n",
    "        print('using CPU, this will be slow!')\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "                                    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "                                    'weight_decay_rate': 0.01},\n",
    "                                    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "                                    'weight_decay_rate': 0.0}\n",
    "                                    ]\n",
    "    optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps = 1e-8)\n",
    "\n",
    "    # BERT training loop\n",
    "    train_loss_set = []\n",
    "    best_f1, best_epoch = 0, 0\n",
    "    for _ in trange(n_epochs, desc=\"Epoch\"):\n",
    "        ###################################################################################\n",
    "        ## TRAINING\n",
    "\n",
    "        # Set our model to training mode\n",
    "        model.train()  \n",
    "        # Tracking variables\n",
    "        tr_loss, tr_perf, tr_perf_classes = 0, Counter({}), pd.DataFrame()\n",
    "        running_len = 0\n",
    "        # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
    "            batch = batch_to_gpu(batch, device)\n",
    "            b_input_ids, b_input_mask, b_labels = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = output[0]\n",
    "            logits = output[1]\n",
    "            print(loss)\n",
    "            train_loss_set.append(loss.item())    \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            print('test')\n",
    "            tmp_tr_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
    "            tmp_tr_perf.update((k,v*len(b_input_ids)) for k,v in tmp_tr_perf.items())\n",
    "            running_len +=len(b_input_ids)\n",
    "            tr_perf = tr_perf + Counter(tmp_tr_perf)\n",
    "            tmp_tr_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
    "            tr_perf_classes = pd.concat((tr_perf_classes, tmp_tr_perf_classes))\n",
    "        \n",
    "        #print('classes detail \\n\\n', tr_perf_classes)\n",
    "        tr_perf = {k:v/running_len for k,v in tr_perf.items()}\n",
    "        tr_perf_classes[['f1-score', 'precision', 'recall']] = tr_perf_classes[['f1-score', 'precision', 'recall']].multiply(tr_perf_classes['support'], axis=\"index\")\n",
    "        tr_perf_classes=tr_perf_classes.groupby(tr_perf_classes.index).sum()\n",
    "        tr_perf_classes[['f1-score','precision', 'recall']] = tr_perf_classes[['f1-score', 'precision', 'recall']].div(tr_perf_classes['support'], axis=\"index\")\n",
    "        #tr_perf_classes=tr_perf_classes.replace(0, np.NaN).groupby(tr_perf_classes.index).agg({'f1-score':'mean','precision':'mean', 'recall':'mean', 'support':'sum'})\n",
    "        print('TRAIN - Loss: {:.3f} - F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(tr_loss/(1+step), tr_perf['f1'], tr_perf['acc'], tr_perf['p'], tr_perf['r']))\n",
    "\n",
    "        ###################################################################################\n",
    "        ## VALIDATION\n",
    "\n",
    "        # Put model in evaluation mode\n",
    "        model.eval()\n",
    "        # Tracking variables \n",
    "        eval_perf, eval_perf_classes = Counter({}), pd.DataFrame()\n",
    "        running_len = 0\n",
    "        # Evaluate data for one epoch\n",
    "        for step, batch in enumerate(validation_dataloader):\n",
    "            # Unpack the inputs from our dataloader (and move to GPU if using)\n",
    "            batch = batch_to_gpu(batch, device)\n",
    "            b_input_ids, b_input_mask, b_labels = batch       \n",
    "            with torch.no_grad(): # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "                # Forward pass, calculate logit predictions\n",
    "                output= model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)  \n",
    "                loss = output[0]\n",
    "                logits = output[1]\n",
    "            # Update tracking variables \n",
    "            tmp_eval_perf = perf_metrics(to_cpu(logits), to_cpu(b_labels), average='weighted')\n",
    "            tmp_eval_perf.update((k,v*len(b_input_ids)) for k,v in tmp_eval_perf.items())\n",
    "            #print('STEP:', step, 'LEN', len(b_input_ids), tmp_eval_perf)\n",
    "            running_len +=len(b_input_ids)\n",
    "            eval_perf = eval_perf + Counter(tmp_eval_perf)\n",
    "            tmp_eval_perf_classes = perf_metrics_classes(to_cpu(logits), to_cpu(b_labels))\n",
    "            eval_perf_classes = pd.concat((eval_perf_classes, tmp_eval_perf_classes))\n",
    "        \n",
    "        eval_perf = {k:v/running_len for k,v in eval_perf.items()}  #eval_perf = {k:v/(1+step) for k,v in eval_perf.items()}\n",
    "        eval_perf_classes[['f1-score', 'precision', 'recall']] = eval_perf_classes[['f1-score', 'precision', 'recall']].multiply(eval_perf_classes['support'], axis=\"index\")\n",
    "        eval_perf_classes=eval_perf_classes.groupby(eval_perf_classes.index).sum()\n",
    "        eval_perf_classes[['f1-score','precision', 'recall']] = eval_perf_classes[['f1-score', 'precision', 'recall']].div(eval_perf_classes['support'], axis=\"index\")\n",
    "        #eval_perf_classes=eval_perf_classes.replace(0, np.NaN).groupby(eval_perf_classes.index).agg({'f1-score':'mean','precision':'mean', 'recall':'mean', 'support':'sum'})\n",
    "        print('TEST -- F1: {:.3f} Acc: {:.3f} P: {:.3f} R: {:.3f}'.format(eval_perf['f1'], eval_perf['acc'], eval_perf['p'], eval_perf['r']))\n",
    "        \n",
    "        # store perf metrics and model\n",
    "        if eval_perf['f1']>=best_f1:\n",
    "            best_f1 = eval_perf['f1']\n",
    "            best_epoch = _ + 1\n",
    "            stats_to_save = eval_perf\n",
    "            tr_perf_classes['dataset'] = 'train'\n",
    "            eval_perf_classes['dataset'] = 'test'\n",
    "            stats_classes_to_save = pd.concat([tr_perf_classes, eval_perf_classes])\n",
    "            model_to_save = model\n",
    "        print('best F1 score obtained: {:.3f} at epoch {}'.format(best_f1, best_epoch))\n",
    "\n",
    "    # save model with best f1\n",
    "    if output_dir is not None:\n",
    "        try:\n",
    "            print('saving model...')\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            stats_classes_to_save.to_csv(output_dir+'/stats.csv', header=True)\n",
    "        except:\n",
    "            print('model not saved, please enter valid path')\n",
    "        \n",
    "    return {'stats':stats_to_save, 'stats_classes':stats_classes_to_save, 'model':model_to_save}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fW0ocHbVOJ61"
   },
   "source": [
    "# Functions to train/evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "M0uTmO7ROPxh"
   },
   "outputs": [],
   "source": [
    "# output_dir='/home/ubuntu/data/ACL2020/bert_models/base')\n",
    "def train_BERT(sentences, labels, BERT_tokenizer='bert-base-uncased', test_size=0.1, \n",
    "               n_epochs=5, batch_size=32, output_dir=None, MAX_TKN_LEN=511):\n",
    "    print('formating dataset')\n",
    "    prep_data = prep_BERT_dataset(sentences=sentences, labels=labels, BERT_tokenizer=BERT_tokenizer, MAX_TKN_LEN=MAX_TKN_LEN)\n",
    "    dataset = prep_data['dataset']\n",
    "    num_labels = prep_data['num_labels']\n",
    "    # split into train/test\n",
    "    print('splitting in train/test sets')\n",
    "    test_len= int(len(dataset)*test_size)\n",
    "    train_len = len(dataset) - test_len\n",
    "    print('test set:', test_len, 'train set:', train_len)\n",
    "    train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_len, test_len])\n",
    "    # Create the DataLoaders for our training and validation sets.\n",
    "    train_dataloader, validation_dataloader = create_dataloader(train_dataset, val_dataset, batch_size = batch_size)\n",
    "    # Load BertForSequenceClassification, the pretrained BERT model with a single linear classification layer on top. \n",
    "    print('loading pre-trained BERT')\n",
    "    pretrained_model = BertForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels=num_labels)\n",
    "    # train and evaluate BERT\n",
    "    print('training BERT')\n",
    "    res = run_BERT(pretrained_model, train_dataloader, validation_dataloader, n_epochs=n_epochs, output_dir=output_dir)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dfn8KqDSFidl"
   },
   "outputs": [],
   "source": [
    "def BERT_KFOLD(sentences, labels, BERT_tokenizer='bert-base-uncased',\n",
    "               n_splits=10, random_state=42, n_epochs=5, output_dir=None, MAX_TKN_LEN=511):\n",
    "    prep_data = prep_BERT_dataset(sentences=sentences, labels=labels, BERT_tokenizer=BERT_tokenizer, MAX_TKN_LEN=MAX_TKN_LEN)\n",
    "    dataset = prep_data['dataset']\n",
    "    num_labels = prep_data['num_labels']\n",
    "    pretrained_model = BertForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels=num_labels)\n",
    "    res = run_KFOLD(dataset=dataset, base_model=BERT_tokenizer, model_trainer=run_BERT, base_model_loader= BertForSequenceClassification.from_pretrained,\n",
    "                    n_splits=n_splits, random_state=random_state, n_epochs=n_epochs, num_labels=prep_data['num_labels'])\n",
    "    if output_dir is not None:\n",
    "        try:\n",
    "            print('saving model in:', output_dir)\n",
    "            res['model'].save_pretrained(output_dir)\n",
    "            res['stats_classes'].to_csv(output_dir+'/stats.csv', header=True)\n",
    "        except:\n",
    "            print('model not saved, please enter valid path')    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "StEAWa_ybD-m"
   },
   "source": [
    "# Function to load saved model and classify new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qyt4FNN9Y4_G"
   },
   "outputs": [],
   "source": [
    "def load_BERT_components(trained_bert_model, BERT_tokenizer='bert-base-uncased', do_lower_case=True):\n",
    "    tokenizer = BertTokenizer.from_pretrained(BERT_tokenizer, do_lower_case=do_lower_case)\n",
    "    trained_bert_model = BertForSequenceClassification.from_pretrained(trained_bert_model)\n",
    "    return {'tokenizer':tokenizer, 'model':trained_bert_model}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lvZDOEfuLIn9"
   },
   "outputs": [],
   "source": [
    "# load pre-trained model and classify a new sentence\n",
    "def load_and_run_BERT(sentences, trained_bert_model, BERT_tokenizer='bert-base-uncased', MAX_TKN_LEN=511, batch_size=32):\n",
    "    if isinstance(BERT_tokenizer, str): # load BERT base model if needed\n",
    "        trained_bert_model = BertForSequenceClassification.from_pretrained(trained_bert_model)\n",
    "    preds_class, probs, preds = [], [], pd.DataFrame()\n",
    "    sentences = pd.Series(sentences)\n",
    "    sentences_dataset = prep_BERT_dataset(sentences, labels=None, BERT_tokenizer=BERT_tokenizer, MAX_TKN_LEN=MAX_TKN_LEN)['dataset']\n",
    "    # b_input_ids, b_input_mask, b_labels = sentences_dataset.tensors\n",
    "    validation_dataloader = DataLoader(sentences_dataset, sampler=SequentialSampler(sentences_dataset), batch_size=batch_size)\n",
    "    for step, batch in enumerate(validation_dataloader):\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            output= trained_bert_model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels, return_dict = False)\n",
    "            loss = output[0]\n",
    "            logits = output[1]\n",
    "        preds_curr = logits.detach().numpy()\n",
    "        preds = preds.append(pd.DataFrame(preds_curr), ignore_index=True)\n",
    "        probs = np.append(probs, np.max(np.exp(preds_curr)/(1+np.exp(preds_curr)), axis=1))\n",
    "        preds_class = np.append(preds_class, np.argmax(preds_curr, axis=1).flatten())\n",
    "    \n",
    "    # put results in nice format\n",
    "    preds['sentences'] = sentences\n",
    "    preds['preds'] = preds_class\n",
    "    preds['probs'] = probs\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1BFc4tjfLIn_"
   },
   "source": [
    "## TESTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u9R_LPhfNmwM"
   },
   "source": [
    "def test():\n",
    "    ####################################################################################\n",
    "    # 0. load pretrained BERT\n",
    "    ####################################################################################\n",
    "    #BERT_tokenizer = 'bert-base-uncased'\n",
    "    #BERT_tokenizer = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "    BERT_tokenizer = 'monologg/biobert_v1.0_pubmed_pmc'\n",
    "\n",
    "    ####################################################################################\n",
    "    # 1. Load Dataset\n",
    "    ####################################################################################\n",
    "    df = pd.read_csv('mimic_status_10folds.csv')[0:100]\n",
    "    #df = pd.read_csv('/home/zeljko_kraljevic/data/F20/annotations/attention_10fold.csv')\n",
    "    #df = pd.read_excel('/home/ZKraljevic/data/covid_anxiety/anxiety_batch_1_riley.xlsx')[0:100]\n",
    "    #df= pd.read_excel('anxiety_batch_2_riley.xlsx')[0:100]\n",
    "    text_col = 'clean_text'\n",
    "    label_col = 'annotation'\n",
    "    df[label_col] = df[label_col].fillna('irrelevant').replace({'irrelevant':0, 'affirmed':1, 'negated':0})\n",
    "    print(df.head())\n",
    "\n",
    "    ####################################################################################\n",
    "    # 2. To run K-fold\n",
    "    ####################################################################################\n",
    "    test = BERT_KFOLD(sentences=df[text_col], labels=df[label_col], n_splits=10, BERT_tokenizer=BERT_tokenizer, n_epochs=5, random_state=666)\n",
    "    #test['model'].save_pretrained('/home/ZKraljevic/data/covid_anxiety/bert_models')\n",
    "    test['model'].save_pretrained('/home/aurelie_mascio/bert_models')\n",
    "\n",
    "    ####################################################################################\n",
    "    # 2b. To run 1 simulation\n",
    "    ####################################################################################\n",
    "    test = train_BERT(sentences=df[text_col], labels=df[label_col], BERT_tokenizer='bert-base-uncased', test_size=0.1, n_epochs=1)\n",
    "    \n",
    "    ####################################################################################\n",
    "    # 3. To test on new data\n",
    "    ####################################################################################\n",
    "    sentences = df.head(5)[text_col] # put your new sentences here\n",
    "    print(sentences)\n",
    "    bert_model_path = 'gdrive/My Drive/Colab Notebooks/prometheus/datasets/bert_models' # '/home/ubuntu/ZKraljevic/covid_anxiety/bert_models/base'\n",
    "    load_and_run_BERT(sentences, trained_bert_model=bert_model_path, BERT_tokenizer='bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from torch.utils.data import DataLoader   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_device():\n",
    "    # specify GPU device\n",
    "    try:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        if device != 'cpu':\n",
    "            print('number GPUs used:',torch.cuda.device_count())\n",
    "            print('device name:', torch.cuda.get_device_name(0))\n",
    "    except:\n",
    "        device = None\n",
    "        print('no CUDA capable device detected')\n",
    "    return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_to_gpu(batch, device=None):\n",
    "    if device is None:\n",
    "        return batch\n",
    "    try: # we're using a GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "    except:\n",
    "        batch = tuple(t for t in batch)\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def perf_metrics(preds, labels, average='weighted', debug=False):\n",
    "#     try:\n",
    "#         pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "#         #pred_flat = torch.max(pred_vec, 1)[1]\n",
    "#     except:\n",
    "#         print('only 1 dimension in labels prediction, no need for argmax')\n",
    "#         pred_flat = preds.flatten()\n",
    "#     labels_flat = labels.flatten()\n",
    "#     acc = accuracy_score(labels_flat, pred_flat)\n",
    "#     f1 = f1_score(labels_flat, pred_flat, average=average)\n",
    "#     p = precision_score(labels_flat, pred_flat, average=average)\n",
    "#     r = recall_score(labels_flat, pred_flat, average=average)\n",
    "#     if debug:\n",
    "#         print(\"PERF -- Acc: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f}\".format(acc,f1,p,r))\n",
    "#     return {'f1':f1, 'acc':acc, 'p':p, 'r':r}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate performance of our predictions vs labels\n",
    "def perf_metrics(preds, labels, average='weighted', debug=False):\n",
    "    try:\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "        #pred_flat = torch.max(pred_vec, 1)[1]\n",
    "    except:\n",
    "        print('only 1 dimension in labels prediction, no need for argmax')\n",
    "        pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    acc = accuracy_score(labels_flat, pred_flat)\n",
    "    f1 = f1_score(labels_flat, pred_flat, average=average)\n",
    "    p = precision_score(labels_flat, pred_flat, average=average)\n",
    "    r = recall_score(labels_flat, pred_flat, average=average)\n",
    "    if debug:\n",
    "        print(\"PERF -- Acc: {:.3f} F1: {:.3f} Precision: {:.3f} Recall: {:.3f}\".format(acc,f1,p,r))\n",
    "    return {'f1':f1, 'acc':acc, 'p':p, 'r':r}\n",
    "\n",
    "def to_cpu(vec, detach=True):\n",
    "    try:\n",
    "        vec = vec.detach().cpu().numpy() if detach else vec.to('cpu').numpy()\n",
    "    except:\n",
    "        vec = vec.detach().numpy() if detach else vec.numpy()\n",
    "    return vec\n",
    "def perf_metrics_classes(preds, labels):\n",
    "    try:\n",
    "        pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    except:\n",
    "        print('only 1 dimension in labels prediction, no need for argmax')\n",
    "        pred_flat = preds.flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    report = classification_report(labels_flat, pred_flat, output_dict=True)\n",
    "    df = pd.DataFrame(report).sort_index().transpose()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['annotation', 'clean_text'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# 0. load pretrained BERT\n",
    "####################################################################################\n",
    "#BERT_tokenizer = 'bert-base-uncased'\n",
    "#BERT_tokenizer = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "#BERT_tokenizer = 'monologg/biobert_v1.0_pubmed_pmc'\n",
    "####################################################################################\n",
    "\n",
    "# 1. Load Dataset\n",
    "####################################################################################\n",
    "#df = pd.read_csv('/home/zeljko_kraljevic/data/F20/annotations/attention_10fold.csv')\n",
    "#df = pd.read_excel('/home/ZKraljevic/data/covid_anxiety/anxiety_batch_1_riley.xlsx')[0:100]\n",
    "df= pd.read_excel('MIMIC_sexual_violence.xlsx')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  annotation                                         clean_text\n",
      "0      Other  The patient gone through a lot of emotional an...\n",
      "1      Other  Psychology to help someone deal with the traum...\n",
      "2   Affirmed  the patient sited emotional, physical abuse, a...\n",
      "3   Affirmed  he said about his abusive experiences, and als...\n",
      "4   Affirmed  The patient mention that the during his childh...\n",
      "(29, 2)\n",
      "            clean_text\n",
      "annotation            \n",
      "Affirmed            17\n",
      "Other               12\n"
     ]
    }
   ],
   "source": [
    "text_col = 'clean_text'\n",
    "label_col = 'annotation'\n",
    "df[label_col] = df[label_col].fillna('irrelevant').replace({'no':0, 'yes':1, 'negated':0})\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "print(df.groupby('annotation').count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelWithLMHead, AutoModel\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "bert = AutoModel.from_pretrained(\"monologg/biobert_v1.0_pubmed_pmc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels have been transformed for the model:\n",
      "\n",
      "           old_label\n",
      "new_label          \n",
      "0          Affirmed\n",
      "1             Other\n",
      "cutting the length of tokens to 511\n"
     ]
    }
   ],
   "source": [
    "####################################################################################\n",
    "# 2. To run K-fold\n",
    "####################################################################################\n",
    "BERT_tokenizer = 'monologg/biobert_v1.0_pubmed_pmc'\n",
    "sentences=df[text_col]\n",
    "labels=df[label_col]\n",
    "n_splits=5\n",
    "BERT_tokenizer=BERT_tokenizer\n",
    "n_epochs=3\n",
    "random_state=666\n",
    "output_dir=None\n",
    "MAX_TKN_LEN=511\n",
    "debug=False\n",
    "\n",
    "# load relevant data and add special tokens for BERT to work properly\n",
    "sentences = [\"[CLS] \" + query + \" [SEP]\" for query in sentences]\n",
    "if labels is not None:\n",
    "    labels = convert_to_cat(labels, binary=False)['labels']\n",
    "else:\n",
    "    labels = pd.Series([1] * len(sentences))\n",
    "if debug: print(sentences[0])\n",
    "\n",
    "# Tokenize with BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(BERT_tokenizer, do_lower_case=True)\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "# seems there are issues for sequence lengths > 512. TO CHECK\n",
    "# https://github.com/huggingface/transformers/issues/2446\n",
    "# !!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "if MAX_TKN_LEN is not None:\n",
    "    print('cutting the length of tokens to', MAX_TKN_LEN)\n",
    "    tokenized_texts = [tokenizer.tokenize(sent)[0:511] for sent in sentences]\n",
    "else:\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts]\n",
    "if debug:\n",
    "    print (\"Tokenize the first sentence:\")\n",
    "    print (len(tokenized_texts), len(input_ids), len(input_ids[0]))\n",
    "    print (tokenized_texts[0], input_ids[0])\n",
    "\n",
    "# add paddding to input_ids\n",
    "input_ids_padded = pad_sequence([torch.tensor(i) for i in input_ids]).transpose(0,1)\n",
    "if debug: print(input_ids_padded.size(), len(input_ids_padded))\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids_padded:\n",
    "    seq_mask = [float(i>0) for i in seq]\n",
    "    attention_masks.append(seq_mask)\n",
    "\n",
    "# create dataset\n",
    "dataset = TensorDataset(input_ids_padded, torch.tensor(attention_masks), torch.tensor(labels))\n",
    "num_labels = labels.nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/biobert_v1.0_pubmed_pmc and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "prep_data={'dataset': dataset, 'num_labels': num_labels}\n",
    "dataset = prep_data['dataset']\n",
    "num_labels = prep_data['num_labels']\n",
    "pretrained_model = BertForSequenceClassification.from_pretrained(BERT_tokenizer, num_labels=num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/biobert_v1.0_pubmed_pmc and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################### RUNNING FOLD: 1\n",
      "<class 'torch.utils.data.dataset.Subset'>  train set: 23  test set: 6\n",
      "training and evaluating model\n",
      "number GPUs used: 0\n",
      "no CUDA capable device detected\n",
      "using CPU, this will be slow!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7272, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.727 - F1: 0.340 Acc: 0.348 P: 0.333 R: 0.348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|████████████▋                         | 1/3 [00:05<00:11,  5.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.250 Acc: 0.333 P: 0.200 R: 0.333\n",
      "best F1 score obtained: 0.250 at epoch 1\n",
      "tensor(0.6832, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.683 - F1: 0.461 Acc: 0.609 P: 0.371 R: 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch:  67%|█████████████████████████▎            | 2/3 [00:11<00:05,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.500 P: 0.250 R: 0.500\n",
      "best F1 score obtained: 0.333 at epoch 2\n",
      "tensor(0.6306, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.631 - F1: 0.440 Acc: 0.565 P: 0.360 R: 0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 100%|██████████████████████████████████████| 3/3 [00:17<00:00,  5.71s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.500 P: 0.250 R: 0.500\n",
      "best F1 score obtained: 0.333 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/biobert_v1.0_pubmed_pmc and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################### RUNNING FOLD: 2\n",
      "<class 'torch.utils.data.dataset.Subset'>  train set: 23  test set: 6\n",
      "training and evaluating model\n",
      "number GPUs used: 0\n",
      "no CUDA capable device detected\n",
      "using CPU, this will be slow!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6867, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.687 - F1: 0.495 Acc: 0.565 P: 0.496 R: 0.565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\r",
      "Epoch:  33%|████████████▋                         | 1/3 [00:05<00:11,  5.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.500 P: 0.250 R: 0.500\n",
      "best F1 score obtained: 0.333 at epoch 1\n",
      "tensor(0.5892, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.589 - F1: 0.563 Acc: 0.609 P: 0.580 R: 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\r",
      "Epoch:  67%|█████████████████████████▎            | 2/3 [00:11<00:05,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.500 P: 0.250 R: 0.500\n",
      "best F1 score obtained: 0.333 at epoch 2\n",
      "tensor(0.5990, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.599 - F1: 0.629 Acc: 0.696 P: 0.797 R: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 100%|██████████████████████████████████████| 3/3 [00:17<00:00,  5.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.500 P: 0.250 R: 0.500\n",
      "best F1 score obtained: 0.333 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/biobert_v1.0_pubmed_pmc and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################### RUNNING FOLD: 3\n",
      "<class 'torch.utils.data.dataset.Subset'>  train set: 23  test set: 6\n",
      "training and evaluating model\n",
      "number GPUs used: 0\n",
      "no CUDA capable device detected\n",
      "using CPU, this will be slow!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6933, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.693 - F1: 0.460 Acc: 0.478 P: 0.513 R: 0.478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|████████████▋                         | 1/3 [00:05<00:10,  5.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.333 P: 0.417 R: 0.333\n",
      "best F1 score obtained: 0.333 at epoch 1\n",
      "tensor(0.6242, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.624 - F1: 0.606 Acc: 0.609 P: 0.605 R: 0.609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|█████████████████████████▎            | 2/3 [00:10<00:05,  5.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.815 Acc: 0.833 P: 0.867 R: 0.833\n",
      "best F1 score obtained: 0.815 at epoch 2\n",
      "tensor(0.6047, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.605 - F1: 0.669 Acc: 0.696 P: 0.725 R: 0.696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████| 3/3 [00:16<00:00,  5.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.815 Acc: 0.833 P: 0.867 R: 0.833\n",
      "best F1 score obtained: 0.815 at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/biobert_v1.0_pubmed_pmc and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################### RUNNING FOLD: 4\n",
      "<class 'torch.utils.data.dataset.Subset'>  train set: 23  test set: 6\n",
      "training and evaluating model\n",
      "number GPUs used: 0\n",
      "no CUDA capable device detected\n",
      "using CPU, this will be slow!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7269, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.727 - F1: 0.328 Acc: 0.435 P: 0.469 R: 0.435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|████████████▋                         | 1/3 [00:05<00:10,  5.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.333 Acc: 0.333 P: 0.417 R: 0.333\n",
      "best F1 score obtained: 0.333 at epoch 1\n",
      "tensor(0.6475, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.647 - F1: 0.652 Acc: 0.652 P: 0.652 R: 0.652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|█████████████████████████▎            | 2/3 [00:11<00:05,  5.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.667 Acc: 0.667 P: 0.667 R: 0.667\n",
      "best F1 score obtained: 0.667 at epoch 2\n",
      "tensor(0.6056, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.606 - F1: 0.781 Acc: 0.783 P: 0.782 R: 0.783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████████████████████████████████| 3/3 [00:17<00:00,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.444 Acc: 0.500 P: 0.400 R: 0.500\n",
      "best F1 score obtained: 0.667 at epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Some weights of the model checkpoint at monologg/biobert_v1.0_pubmed_pmc were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at monologg/biobert_v1.0_pubmed_pmc and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####################### RUNNING FOLD: 5\n",
      "<class 'torch.utils.data.dataset.Subset'>  train set: 24  test set: 5\n",
      "training and evaluating model\n",
      "number GPUs used: 0\n",
      "no CUDA capable device detected\n",
      "using CPU, this will be slow!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|                                              | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7016, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.702 - F1: 0.538 Acc: 0.542 P: 0.535 R: 0.542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\r",
      "Epoch:  33%|████████████▋                         | 1/3 [00:05<00:11,  5.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.450 Acc: 0.600 P: 0.360 R: 0.600\n",
      "best F1 score obtained: 0.450 at epoch 1\n",
      "tensor(0.6777, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.678 - F1: 0.583 Acc: 0.583 P: 0.583 R: 0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "\r",
      "Epoch:  67%|█████████████████████████▎            | 2/3 [00:11<00:05,  5.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.450 Acc: 0.600 P: 0.360 R: 0.600\n",
      "best F1 score obtained: 0.450 at epoch 2\n",
      "tensor(0.6374, grad_fn=<NllLossBackward0>)\n",
      "test\n",
      "TRAIN - Loss: 0.637 - F1: 0.682 Acc: 0.708 P: 0.732 R: 0.708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Epoch: 100%|██████████████████████████████████████| 3/3 [00:17<00:00,  5.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TEST -- F1: 0.450 Acc: 0.600 P: 0.360 R: 0.600\n",
      "best F1 score obtained: 0.450 at epoch 3\n",
      "best F1 score obtained across splits: 0.815\n"
     ]
    }
   ],
   "source": [
    "res = run_KFOLD(dataset=dataset, base_model=BERT_tokenizer, model_trainer=run_BERT, base_model_loader= BertForSequenceClassification.from_pretrained, n_splits=n_splits, random_state=random_state, n_epochs=n_epochs, num_labels=prep_data['num_labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving model in: /Users/lilifang/KCL/KCL_Angus/CRIS_COVID_Aurelie/MiMiC_sexual_model_LF\n"
     ]
    }
   ],
   "source": [
    "output_dir= '/Users/lilifang/KCL/KCL_Angus/CRIS_COVID_Aurelie/MiMiC_sexual_model_LF'\n",
    "\n",
    "try:\n",
    "    print('saving model in:', output_dir)\n",
    "    res['model'].save_pretrained(output_dir)\n",
    "    res['stats_classes'].to_csv(output_dir+'/stats.csv', header=True)\n",
    "except:\n",
    "    print('model not saved, please enter valid path')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "BERT.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
